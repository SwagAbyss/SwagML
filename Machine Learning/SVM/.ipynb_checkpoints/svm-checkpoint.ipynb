{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Explain various kernel functions in SVM.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Support Vector Machines (SVMs) are a type of supervised learning algorithm that can be used for classification and regression tasks. One of the key features of SVMs is the ability to use a kernel function to transform the data into a higher dimensional space, where it may be easier to find a linear boundary between the classes.\n",
    "\n",
    "- The following are some common kernel functions used in SVMs:\n",
    "\n",
    "__1. Linear kernel:__\n",
    "- This kernel function simply maps the input data to a higher dimensional space where it can be separated by a linear boundary. \n",
    "- The linear kernel is the simplest and most computationally efficient kernel function, but it may not be able to find a suitable boundary when the classes are not linearly separable.\n",
    "\n",
    "        from sklearn.svm import SVC\n",
    "        clf = SVC(kernel='linear')\n",
    "\n",
    "\n",
    "__2. Polynomial kernel:__ \n",
    "- This kernel function maps the input data to a higher dimensional space where it can be separated by a non-linear boundary. \n",
    "- It is defined as (1+x^T.y)^d, where d is the degree of the polynomial. \n",
    "- The polynomial kernel can be useful when the classes are not linearly separable, but it can also be computationally expensive for high-dimensional data.\n",
    "\n",
    "        from sklearn.svm import SVC\n",
    "        clf = SVC(kernel='poly', degree=3)\n",
    "\n",
    "\n",
    "__3. Radial Basis Function (RBF) kernel:__ \n",
    "- This kernel function maps the input data to a higher dimensional space where it can be separated by a non-linear boundary. \n",
    "- It is defined as exp(-gamma*||x-y||^2), where gamma is a parameter that controls the width of the radial basis function. \n",
    "- The RBF kernel is a popular choice for non-linear classification tasks and it's less sensitive to the choice of parameters than the polynomial kernel.\n",
    "\n",
    "        from sklearn.svm import SVC\n",
    "        clf = SVC(kernel='rbf', gamma=0.1)\n",
    "\n",
    "\n",
    "__4. Sigmoid kernel:__ \n",
    "- This kernel function maps the input data to a higher dimensional space where it can be separated by a non-linear boundary. \n",
    "- It is defined as tanh(gamma*x^T.y+c). \n",
    "- The sigmoid kernel is less popular than the other kernels but it can be useful for certain types of datasets.\n",
    "\n",
    "        from sklearn.svm import SVC\n",
    "        clf = SVC(kernel='sigmoid', alpha=0.1, r=0.1)\n",
    "\n",
    "\n",
    "__5. Laplacian kernel:__ \n",
    "- Laplacian kernel also known as the exponential kernel, it is defined as exp(-gamma*||x-y||) and it's useful for datasets that have a high degree of similarity between the classes.\n",
    "\n",
    "It's important to note that the choice of kernel function depends on the specific dataset and problem, and that the linear kernel should be tried first as it's the fastest and the simplest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Explain difference between hard and soft margin.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Support Vector Machines (SVMs), the goal is to find the best hyperplane that separates the data into different classes. \n",
    "The margin is the distance between the hyperplane and the closest data points, which are called support vectors.\n",
    "\n",
    "- The difference between hard and soft margin lies in how the algorithm handles observations that are not correctly classified:\n",
    "\n",
    "__1. Hard margin:__\n",
    "-  In hard margin, the algorithm only considers observations that are correctly classified and tries to maximize the margin. \n",
    "-  This means that it allows no misclassifications. However, this approach works only when the data is linearly separable.\n",
    "\n",
    "        from sklearn.svm import SVC\n",
    "        clf = SVC(C=float(\"inf\"), kernel='linear')\n",
    "\n",
    "\n",
    "**2. Soft margin:**\n",
    "- In soft margin, the algorithm allows some misclassifications, and it tries to find a balance between maximizing the margin and minimizing the number of misclassifications. \n",
    "- This approach works when the data is not linearly separable. \n",
    "- The parameter C in the SVC class is used to control the trade-off between maximizing the margin and minimizing the misclassifications. \n",
    "- A smaller value of C means a larger margin and more misclassifications, while a larger value of C means a smaller margin and fewer misclassifications.\n",
    "\n",
    "        from sklearn.svm import SVC\n",
    "        clf = SVC(C=1, kernel='linear')\n",
    "\n",
    "It's important to note that in real-world datasets, it is rare to have a linearly separable data, and Soft Margin is the more common approach to use."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
